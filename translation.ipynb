{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filename, delimiter=\"\\n\", names=['text'])\n",
    "\n",
    "english_full = load_data(\"data/small_vocab_en\")\n",
    "french_full = load_data(\"data/small_vocab_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text\n",
      "541  his most loved fruit is the lime , but her mos...\n",
      "542  the grapefruit is your least liked fruit , but...\n",
      "543            she was driving that shiny blue truck .\n",
      "544  paris is never freezing during july , but it i...\n",
      "545  california is sometimes cold during autumn , b...\n",
      "546  california is sometimes warm during february ,...\n",
      "547  paris is usually busy during june , and it is ...\n",
      "548  the united states is sometimes nice during jul...\n",
      "549  china is cold during december , and it is neve...\n",
      "550  india is never dry during spring , but it is u...\n",
      "551  the strawberry is our least liked fruit , but ...\n",
      "552  the lemon is my least favorite fruit , but the...\n",
      "                                                  text\n",
      "541  son fruit le plus aimé est la chaux , mais son...\n",
      "542  le pamplemousse est votre fruit moins aimé , m...\n",
      "543          elle conduisait ce camion bleu brillant .\n",
      "544  paris est jamais le gel en juillet , mais il e...\n",
      "545  californie est parfois froid pendant l' automn...\n",
      "546  californie est parfois chaud en février , et i...\n",
      "547  paris est généralement occupé en juin , et il ...\n",
      "548  les états-unis est parfois agréable en juillet...\n",
      "549  chine est froid en décembre , et il est jamais...\n",
      "550  l' inde est jamais à sec au printemps , mais i...\n",
      "551  la fraise est notre fruit moins aimé , mais la...\n",
      "552  le citron est mon fruit préféré moins , mais l...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "137860"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(english_full.iloc[541:553,])\n",
    "print(french_full.iloc[541:553,])\n",
    "len(english_full)\n",
    "len(french_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english, english_test, french, french_test = train_test_split(english_full, french_full, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59175           [she, dislikes, apples, and, strawberries]\n",
       "22551           [she, likes, bananas, limes, and, oranges]\n",
       "62827    [the, united, states, is, freezing, during, se...\n",
       "8733     [california, is, snowy, during, february, and,...\n",
       "1244     [new, jersey, is, never, pleasant, during, sum...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import contractions\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "def preprocess_text(corpus, language):\n",
    "    corpus = corpus['text'].str.lower()\n",
    "    corpus = corpus.map(lambda x: x.translate(str.maketrans('', '', punctuation)))\n",
    "    if language == 'english':\n",
    "        corpus = corpus.map(lambda x: contractions.fix(x))\n",
    "    corpus = corpus.map(word_tokenize)\n",
    "    return corpus\n",
    "\n",
    "en_preprocessed = preprocess_text(english, 'english')\n",
    "fr_preprocessed = preprocess_text(french, 'french')\n",
    "\n",
    "en_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def word_to_id(token_list):\n",
    "    '''creates a dictionary mapping each unique word to a unique integer id'''\n",
    "    id_dict = defaultdict((count().__next__))\n",
    "    for token in token_list:\n",
    "        id_dict[token] = id_dict[token]\n",
    "    return id_dict\n",
    "\n",
    "\n",
    "def tokens_to_sequence(tokens):\n",
    "    '''takes a dataframe with a \"text\" column and maps the text column to integers'''\n",
    "    tokenlist = []\n",
    "    for token in tokens:\n",
    "        tokenlist.extend(token)\n",
    "    id_dict = word_to_id(list(set(tokenlist)))\n",
    "    sequence_list = tokens.map(lambda x: [id_dict[w] + 1 for w in x])\n",
    "    return sequence_list, id_dict\n",
    "\n",
    "\n",
    "def test_token_update(tokens, id_dict):\n",
    "    '''takes a dataframe with a \"text\" column and a dictionary mapping words to ids. \n",
    "    Maps the text column to integers based on said dictionary'''\n",
    "    sequence_list = tokens.map(lambda x: [id_dict[w] + 1 if w in id_dict.keys() else 0 for w in x])\n",
    "    return sequence_list\n",
    "\n",
    "\n",
    "def pad_sequence(sequence):\n",
    "    '''takes an array of text and pads the sequence with 0s so all inputs are the same length'''\n",
    "    return pad_sequences(sequence, len(max(sequence, key=len)), 'int', 'post', 'post', 0)\n",
    "\n",
    "\n",
    "def pad_test_sequence(sequence, length = None):\n",
    "    '''takes an array of text and a padded sequence, pads the array of text to the same length as the padded sequence'''\n",
    "    if length:\n",
    "        return pad_sequences(sequence, length, 'int', 'post', 'post', 0)\n",
    "    return pad_sequences(sequence, len(max(train_sequence, key=len)), 'int', 'post', 'post', 0)\n",
    "\n",
    "\n",
    "en_sequence, en_id_dict = tokens_to_sequence(en_preprocessed)\n",
    "fr_sequence, fr_id_dict = tokens_to_sequence(fr_preprocessed)\n",
    "\n",
    "en_padded = pad_sequence(en_sequence)\n",
    "fr_padded = pad_sequence(fr_sequence)\n",
    "\n",
    "en_len = en_padded.shape[1]\n",
    "fr_len = fr_padded.shape[1]\n",
    "en_vocab = max(en_id_dict.values()) + 1\n",
    "fr_vocab = max(fr_id_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_text(output, id_dict):\n",
    "    idtt = {v + 1:k for k, v in id_dict.items()}\n",
    "    idtt[0] = '<EMPTY>'\n",
    "    idtt[fr_vocab+1] = '<START>'\n",
    "    \n",
    "    return ' '.join(idtt[word] for word in np.argmax(output, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 21 197 343\n"
     ]
    }
   ],
   "source": [
    "print(en_len, fr_len, en_vocab, fr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 250)           49500     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 21, 512)           1038336   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 1024)          525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 21, 344)           352600    \n",
      "=================================================================\n",
      "Total params: 1,965,748\n",
      "Trainable params: 1,965,748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, GRU, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "fr_padded_3d = fr_padded.reshape(*fr_padded.shape, 1)\n",
    "\n",
    "tmp_x = pad_test_sequence(en_padded, fr_padded_3d.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, fr_padded_3d.shape[-2]))\n",
    "\n",
    "\n",
    "def compile_embedding_RNN(input_length, input_vocab_size, output_vocab_size, bidirectional = True, embed_size=200, \n",
    "                          lstm_size=256, lstm_dropout=.2, dense_size=1024, dense_dropout=.5, learning_rate=.001):\n",
    "    '''compiles an RNN model.\n",
    "    Input Length: The length of the input sequences\n",
    "    Input vocab size: The size of the vocabulary for the input data\n",
    "    Output vocab size: The size of the vocabulary for the labels\n",
    "    Bidirectional: Should the RNN use a bidirectional layer?\n",
    "    Embed size: the size of the embedding layer\n",
    "    LSTM size: the size of the LSTM layer\n",
    "    LSTM dropout: The amount of dropout to apply to the LSTM layer. Must be a number between 0 and 1.\n",
    "    Dense size: the size of the Dense layer\n",
    "    Dense dropout: The amount of dropout to apply to the dense layer. Must be an umber between 0 and 1.\n",
    "    Learning rate: A small number, usually around .001 or .01'''\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_vocab_size, embed_size, input_length=input_length,  input_shape=(input_length, )))\n",
    "    if bidirectional:\n",
    "        model.add(Bidirectional(LSTM(lstm_size, activation='tanh', recurrent_activation='sigmoid', dropout=lstm_dropout, \n",
    "                       return_sequences = True)))\n",
    "    else:\n",
    "        model.add(LSTM(lstm_size, activation='tanh', recurrent_activation='sigmoid', dropout=lstm_dropout, \n",
    "                       return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(dense_size, activation='relu')))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "    model.add(TimeDistributed(Dense(output_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss = sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "embedding_size = 250\n",
    "lstm_size = 256\n",
    "lstm_dropout = .5\n",
    "dense_size = 1024\n",
    "dense_dropout = .5\n",
    "learning_rate = .003\n",
    "\n",
    "model = compile_embedding_RNN(fr_len, en_vocab+1, fr_vocab+1, True, embedding_size, lstm_size, lstm_dropout, dense_size, \n",
    "                              dense_dropout, learning_rate)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99259 samples, validate on 24815 samples\n",
      "Epoch 1/5\n",
      "99259/99259 [==============================] - 255s 3ms/step - loss: 0.1994 - accuracy: 0.9358 - val_loss: 0.1304 - val_accuracy: 0.9573\n",
      "Epoch 2/5\n",
      "99259/99259 [==============================] - 273s 3ms/step - loss: 0.1510 - accuracy: 0.9515 - val_loss: 0.1020 - val_accuracy: 0.9665\n",
      "Epoch 3/5\n",
      "99259/99259 [==============================] - 283s 3ms/step - loss: 0.1210 - accuracy: 0.9612 - val_loss: 0.0863 - val_accuracy: 0.9721\n",
      "Epoch 4/5\n",
      "99259/99259 [==============================] - 220s 2ms/step - loss: 0.1069 - accuracy: 0.9658 - val_loss: 0.0800 - val_accuracy: 0.9755\n",
      "Epoch 5/5\n",
      "99259/99259 [==============================] - 244s 2ms/step - loss: 0.0911 - accuracy: 0.9708 - val_loss: 0.0661 - val_accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = tmp_x, y = fr_padded_3d, epochs = 5, batch_size = 250, verbose = 1, validation_split = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_text(model.predict(tmp_x[:1])[0], fr_id_dict)\n",
    "\n",
    "model.save('baseline.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconditioned Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "\n",
    "def compile_enc_dec_RNN(input_length, input_vocab_size, output_vocab_size, embed_size=200, \n",
    "                          lstm_size=256, lstm_dropout=.2, dense_size=1024, dense_dropout=.5, learning_rate=.001):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_vocab_size, embed_size, input_length=input_length,  input_shape=(input_length, )))\n",
    "    model.add(Bidirectional(LSTM(lstm_size, activation='tanh', recurrent_activation='sigmoid', dropout=lstm_dropout)))\n",
    "    model.add(RepeatVector(fr_len))\n",
    "    model.add(Bidirectional(LSTM(lstm_size, activation='tanh', recurrent_activation='sigmoid', dropout=lstm_dropout, \n",
    "                                 return_sequences = True)))\n",
    "    model.add(TimeDistributed(Dense(dense_size, activation='relu')))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "    model.add(TimeDistributed(Dense(output_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss = sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 15, 200)           39600     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               935936    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 21, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 1024)          525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 344)           352600    \n",
      "=================================================================\n",
      "Total params: 3,428,360\n",
      "Trainable params: 3,428,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = compile_enc_dec_RNN(en_len, en_vocab+1, fr_vocab+1, learning_rate = .003)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sam\\.virtualenvs\\machine_translation-bsvnioog\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99259 samples, validate on 24815 samples\n",
      "Epoch 1/5\n",
      "99259/99259 [==============================] - 578s 6ms/step - loss: 1.0442 - accuracy: 0.7096 - val_loss: 0.5159 - val_accuracy: 0.8318\n",
      "Epoch 2/5\n",
      "99259/99259 [==============================] - 550s 6ms/step - loss: 0.3979 - accuracy: 0.8770 - val_loss: 0.1915 - val_accuracy: 0.9407\n",
      "Epoch 3/5\n",
      "99259/99259 [==============================] - 598s 6ms/step - loss: 0.2282 - accuracy: 0.9307 - val_loss: 0.1372 - val_accuracy: 0.9570\n",
      "Epoch 4/5\n",
      "99259/99259 [==============================] - 612s 6ms/step - loss: 0.1734 - accuracy: 0.9473 - val_loss: 0.1073 - val_accuracy: 0.9664\n",
      "Epoch 5/5\n",
      "99259/99259 [==============================] - 657s 7ms/step - loss: 0.1433 - accuracy: 0.9562 - val_loss: 0.0929 - val_accuracy: 0.9709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1aeb1a099b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(en_padded, fr_padded_3d, epochs = 5, batch_size = 64, verbose = 1, validation_split = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_text(model2.predict(en_padded[:1])[0], fr_id_dict)\n",
    "model2.save('unconditioned_encoder_decoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioned Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = np.zeros((en_sequence.shape[0], fr_len), dtype = 'uint16')\n",
    "fr_padded_3d = np.zeros((en_sequence.shape[0], fr_len, fr_vocab+2), dtype = 'uint16')\n",
    "for i, sequence in enumerate(fr_padded):\n",
    "    for j, word in enumerate(sequence):\n",
    "        fr_padded_3d[i, j, word] = 1\n",
    "        if j == 0:\n",
    "            decoder_input_data[i, j] = fr_vocab + 1\n",
    "        else:\n",
    "            decoder_input_data[i, j] = fr_padded[i, j-1]\n",
    "            \n",
    "en_padded = pad_sequences(en_padded, en_len + 1, padding = 'pre', value = en_vocab+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124074, 21, 346)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_padded_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_models(n_input, n_output, vec_len, n_units):\n",
    "    '''\n",
    "    n_input: Number of first language words\n",
    "    n_output: Number of second language words\n",
    "    vec_len: Length of embedding vectors\n",
    "    n_units: Number of hidden units\n",
    "    '''\n",
    "    encoder_inputs = Input(shape = (None,))\n",
    "    enc_embed = Embedding(n_input, vec_len)\n",
    "    encoder = LSTM(n_units, return_state = True)\n",
    "    encoder_outputs, state_h, state_c = encoder(enc_embed(encoder_inputs))\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    dec_embed = Embedding(n_output, vec_len)\n",
    "    decoder_lstm = LSTM(n_units, return_sequences = True, return_state = True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_embed(decoder_inputs), initial_state = encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation = 'softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(dec_embed(decoder_inputs), initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "model3, enc_model, dec_model = define_models(en_vocab+1, fr_vocab+2, 100, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99259 samples, validate on 24815 samples\n",
      "Epoch 1/20\n",
      "99259/99259 [==============================] - 143s 1ms/step - loss: 1.8875 - accuracy: 0.5727 - val_loss: 1.0147 - val_accuracy: 0.6964\n",
      "Epoch 2/20\n",
      "99259/99259 [==============================] - 143s 1ms/step - loss: 0.7959 - accuracy: 0.7373 - val_loss: 0.7082 - val_accuracy: 0.7534\n",
      "Epoch 3/20\n",
      "99259/99259 [==============================] - 146s 1ms/step - loss: 0.6344 - accuracy: 0.7744 - val_loss: 0.6007 - val_accuracy: 0.7801\n",
      "Epoch 4/20\n",
      "99259/99259 [==============================] - 145s 1ms/step - loss: 0.5690 - accuracy: 0.7927 - val_loss: 0.5584 - val_accuracy: 0.7982\n",
      "Epoch 5/20\n",
      "99259/99259 [==============================] - 143s 1ms/step - loss: 0.5302 - accuracy: 0.8044 - val_loss: 0.5249 - val_accuracy: 0.8041\n",
      "Epoch 6/20\n",
      "99259/99259 [==============================] - 143s 1ms/step - loss: 0.5015 - accuracy: 0.8147 - val_loss: 0.4833 - val_accuracy: 0.8217\n",
      "Epoch 7/20\n",
      "99259/99259 [==============================] - 144s 1ms/step - loss: 0.4657 - accuracy: 0.8277 - val_loss: 0.4552 - val_accuracy: 0.8306\n",
      "Epoch 8/20\n",
      "99259/99259 [==============================] - 145s 1ms/step - loss: 0.4194 - accuracy: 0.8450 - val_loss: 0.3960 - val_accuracy: 0.8526\n",
      "Epoch 9/20\n",
      "99259/99259 [==============================] - 146s 1ms/step - loss: 0.3713 - accuracy: 0.8640 - val_loss: 0.3509 - val_accuracy: 0.8703\n",
      "Epoch 10/20\n",
      "99259/99259 [==============================] - 148s 1ms/step - loss: 0.3225 - accuracy: 0.8825 - val_loss: 0.3036 - val_accuracy: 0.8883\n",
      "Epoch 11/20\n",
      "99259/99259 [==============================] - 147s 1ms/step - loss: 0.2726 - accuracy: 0.9015 - val_loss: 0.2459 - val_accuracy: 0.9116\n",
      "Epoch 12/20\n",
      "99259/99259 [==============================] - 147s 1ms/step - loss: 0.2201 - accuracy: 0.9200 - val_loss: 0.1928 - val_accuracy: 0.9307\n",
      "Epoch 13/20\n",
      "99259/99259 [==============================] - 147s 1ms/step - loss: 0.1701 - accuracy: 0.9396 - val_loss: 0.1478 - val_accuracy: 0.9490\n",
      "Epoch 14/20\n",
      "99259/99259 [==============================] - 145s 1ms/step - loss: 0.1250 - accuracy: 0.9579 - val_loss: 0.1022 - val_accuracy: 0.9682\n",
      "Epoch 15/20\n",
      "99259/99259 [==============================] - 142s 1ms/step - loss: 0.0894 - accuracy: 0.9719 - val_loss: 0.0829 - val_accuracy: 0.9733\n",
      "Epoch 16/20\n",
      "99259/99259 [==============================] - 140s 1ms/step - loss: 0.0660 - accuracy: 0.9797 - val_loss: 0.0587 - val_accuracy: 0.9817\n",
      "Epoch 17/20\n",
      "99259/99259 [==============================] - 143s 1ms/step - loss: 0.0514 - accuracy: 0.9840 - val_loss: 0.0488 - val_accuracy: 0.9837\n",
      "Epoch 18/20\n",
      "99259/99259 [==============================] - 144s 1ms/step - loss: 0.0405 - accuracy: 0.9875 - val_loss: 0.0372 - val_accuracy: 0.9884\n",
      "Epoch 19/20\n",
      "99259/99259 [==============================] - 141s 1ms/step - loss: 0.0334 - accuracy: 0.9896 - val_loss: 0.0308 - val_accuracy: 0.9904\n",
      "Epoch 20/20\n",
      "99259/99259 [==============================] - 145s 1ms/step - loss: 0.0280 - accuracy: 0.9913 - val_loss: 0.0295 - val_accuracy: 0.9905\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model3.fit([en_padded, decoder_input_data], fr_padded_3d,\n",
    "          batch_size=500,\n",
    "          epochs=20,\n",
    "          validation_split=0.2)\n",
    "model3.save('conditioned_encoder_decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 55  92 111   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['jaime',\n",
       "  'les',\n",
       "  'pommes',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>',\n",
       "  '<EMPTY>']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idtt = {v + 1:k for k, v in fr_id_dict.items()}\n",
    "idtt[0] = '<EMPTY>'\n",
    "idtt[fr_vocab + 1] = '<START>'\n",
    "    \n",
    "def decode_sequence(input_seq):\n",
    "    '''Takes an input sequence and predicts the output sequence'''\n",
    "    # Encode the input as state vectors.\n",
    "    tokens = preprocess_text(input_seq, 'english')\n",
    "    sequence = test_token_update(tokens, en_id_dict)\n",
    "    encoder_input = pad_test_sequence(sequence, en_len)\n",
    "    states_value = enc_model.predict(encoder_input)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = fr_vocab + 1\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_df = []\n",
    "    \n",
    "    for i, sequence in enumerate(encoder_input):\n",
    "        decoded_sentence = []\n",
    "        for j in range(len(sequence)):\n",
    "            output_tokens, h, c = dec_model.predict([target_seq] + states_value)\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_word = idtt[sampled_token_index]\n",
    "            decoded_sentence.append(sampled_word)\n",
    "            target_seq = np.zeros((1,1))\n",
    "            target_seq[0,0] = sampled_token_index\n",
    "            states_value = [h,c]\n",
    "        decoded_df.append(decoded_sentence)\n",
    "        \n",
    "    return decoded_df\n",
    "\n",
    "decode_df = pd.DataFrame({\"text\": [\"I like apples\"]})\n",
    "decode_sequence(decode_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
